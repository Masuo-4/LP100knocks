{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ec4f62-98e2-428b-b272-be96ec9e61c9",
   "metadata": {},
   "source": [
    "<h2>第六章 機械学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42cfb1",
   "metadata": {},
   "source": [
    "本章では，Fabio Gasparetti氏が公開しているNews Aggregator Data Setを用い，ニュース記事の見出しを「ビジネス」「科学技術」「エンターテイメント」「健康」のカテゴリに分類するタスク（カテゴリ分類）に取り組む．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bb9e393f-4f74-4656-8338-9d9d4e275f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
    "# !unzip NewsAggregatorDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c648fce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  422937 ./data/NewsAggregatorDataset/newsCorpora.csv\n",
      "1\tFed official says weak data caused by weather, should not slow taper\thttp://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310,0,1312750.story\\?track=rss\tLos Angeles Times\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.latimes.com\t1394470370698\n",
      "2\tFed's Charles Plosser sees high bar for change in pace of tapering\thttp://www.livemint.com/Politics/H2EvwJSK2VE6OF7iK1g3PP/Feds-Charles-Plosser-sees-high-bar-for-change-in-pace-of-ta.html\tLivemint\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.livemint.com\t1394470371207\n",
      "3\tUS open: Stocks fall after Fed official hints at accelerated tapering\thttp://www.ifamagazine.com/news/us-open-stocks-fall-after-fed-official-hints-at-accelerated-tapering-294436\tIFA Magazine\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.ifamagazine.com\t1394470371550\n",
      "4\tFed risks falling 'behind the curve', Charles Plosser says\thttp://www.ifamagazine.com/news/fed-risks-falling-behind-the-curve-charles-plosser-says-294430\tIFA Magazine\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.ifamagazine.com\t1394470371793\n",
      "5\tFed's Plosser: Nasty Weather Has Curbed Job Growth\thttp://www.moneynews.com/Economy/federal-reserve-charles-plosser-weather-job-growth/2014/03/10/id/557011\tMoneynews\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.moneynews.com\t1394470372027\n",
      "6\tPlosser: Fed May Have to Accelerate Tapering Pace\thttp://www.nasdaq.com/article/plosser-fed-may-have-to-accelerate-tapering-pace-20140310-00371\tNASDAQ\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.nasdaq.com\t1394470372212\n",
      "7\tFed's Plosser: Taper pace may be too slow\thttp://www.marketwatch.com/story/feds-plosser-taper-pace-may-be-too-slow-2014-03-10\\?reflink=MW_news_stmp\tMarketWatch\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.marketwatch.com\t1394470372405\n",
      "8\tFed's Plosser expects US unemployment to fall to 6.2% by the end of 2014\thttp://www.fxstreet.com/news/forex-news/article.aspx\\?storyid=23285020-b1b5-47ed-a8c4-96124bb91a39\tFXstreet.com\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.fxstreet.com\t1394470372615\n",
      "9\tUS jobs growth last month hit by weather:Fed President Charles Plosser\thttp://economictimes.indiatimes.com/news/international/business/us-jobs-growth-last-month-hit-by-weatherfed-president-charles-plosser/articleshow/31788000.cms\tEconomic Times\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\teconomictimes.indiatimes.com\t1394470372792\n",
      "10\tECB unlikely to end sterilisation of SMP purchases - traders\thttp://www.iii.co.uk/news-opinion/reuters/news/152615\tInteractive Investor\tb\tdPhGU51DcrolUIMxbRm0InaHGA2XM\twww.iii.co.uk\t1394470501265\n"
     ]
    }
   ],
   "source": [
    "# 行数の確認\n",
    "!wc -l ./data/NewsAggregatorDataset/newsCorpora.csv\n",
    "# 先頭10行の確認\n",
    "!head -10 ./data/NewsAggregatorDataset/newsCorpora.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2e86b",
   "metadata": {},
   "source": [
    "<h3>50: News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "\n",
    "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
    "2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "3. 抽出された事例をランダムに並び替える．\n",
    "4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "805e6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
    "# !sed -e 's/\"/'\\''/g' ./data/NewsAggregatorDataset/newsCorpora.csv > ./data/NewsAggregatorDataset/newsCorpora_re.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee79fe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【学習データ】\n",
      "b    4501\n",
      "e    4235\n",
      "t    1220\n",
      "m     728\n",
      "Name: CATEGORY, dtype: int64\n",
      "【検証データ】\n",
      "b    563\n",
      "e    529\n",
      "t    153\n",
      "m     91\n",
      "Name: CATEGORY, dtype: int64\n",
      "【評価データ】\n",
      "b    563\n",
      "e    530\n",
      "t    152\n",
      "m     91\n",
      "Name: CATEGORY, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# データの読み込み\n",
    "df = pd.read_csv('data/NewsAggregatorDataset/newsCorpora_re.csv', header=None, sep='\\t')\n",
    "df.columns = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "\n",
    "# データの抽出\n",
    "publisher_lst = [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]\n",
    "df = df.query(\"PUBLISHER in @publisher_lst\").filter([\"TITLE\", \"CATEGORY\"])\n",
    "\n",
    "# データの分割\n",
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n",
    "\n",
    "# データの保存\n",
    "train.to_csv('./data/NewsAggregatorDataset/train.txt', sep='\\t', index=False)\n",
    "valid.to_csv('./data/NewsAggregatorDataset/valid.txt', sep='\\t', index=False)\n",
    "test.to_csv('./data/NewsAggregatorDataset/test.txt', sep='\\t', index=False)\n",
    "\n",
    "# 事例数の確認\n",
    "print('【学習データ】')\n",
    "print(train['CATEGORY'].value_counts())\n",
    "print('【検証データ】')\n",
    "print(valid['CATEGORY'].value_counts())\n",
    "print('【評価データ】')\n",
    "print(test['CATEGORY'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0615981",
   "metadata": {},
   "source": [
    "<h3>51: 学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15219f7",
   "metadata": {},
   "source": [
    "前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ceb06c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>refile update 0 european car sales up for sixt...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon plans to fight ftc over mobile app purc...</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kids still get codeine in emergency rooms desp...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what on earth happened between solange and jay...</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nato missile defense is flight tested over hawaii</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY\n",
       "0  refile update 0 european car sales up for sixt...        b\n",
       "1  amazon plans to fight ftc over mobile app purc...        t\n",
       "2  kids still get codeine in emergency rooms desp...        m\n",
       "3  what on earth happened between solange and jay...        e\n",
       "4  nato missile defense is flight tested over hawaii        b"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 最初と同じ処理\n",
    "df = pd.read_csv('data/NewsAggregatorDataset/newsCorpora_re.csv', header=None, sep='\\t')\n",
    "df.columns = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\n",
    "publisher_lst = [\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]\n",
    "df = df.query(\"PUBLISHER in @publisher_lst\").filter([\"TITLE\", \"CATEGORY\"])\n",
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n",
    "\n",
    "def preprocessing(text):\n",
    "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "  text = text.translate(table)  # 記号をスペースに置換\n",
    "  text = text.lower()  # 小文字化\n",
    "  text = re.sub('[0-9]+', '0', text)  # 数字列を0に置換\n",
    "\n",
    "  return text\n",
    "\n",
    "# データの再結合\n",
    "df = pd.concat([train, valid, test], axis=0)\n",
    "df.reset_index(drop=True, inplace=True)  # indexを振りなおす\n",
    "df[\"TITLE\"] = df[\"TITLE\"].apply(preprocessing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0784fca",
   "metadata": {},
   "source": [
    "記事の見出しをスペースで分割した単語群を対象にTF-IDFを算出し、その値を特徴量として利用することにします。また、1単語(uni-gram)だけでなく連続する2単語(bi-gram)についてもTF-IDFを計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3eb0c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0m  0million  0nd   0s  0st  0th  0th birthday   aa  aaliyah  abbvie  ...  \\\n",
      "0  0.0       0.0  0.0  0.0  0.0  0.0           0.0  0.0      0.0     0.0  ...   \n",
      "1  0.0       0.0  0.0  0.0  0.0  0.0           0.0  0.0      0.0     0.0  ...   \n",
      "2  0.0       0.0  0.0  0.0  0.0  0.0           0.0  0.0      0.0     0.0  ...   \n",
      "3  0.0       0.0  0.0  0.0  0.0  0.0           0.0  0.0      0.0     0.0  ...   \n",
      "4  0.0       0.0  0.0  0.0  0.0  0.0           0.0  0.0      0.0     0.0  ...   \n",
      "\n",
      "   young  your  your mother   yr  yr high  yuan  zac  zac efron  zendaya  zone  \n",
      "0    0.0   0.0          0.0  0.0      0.0   0.0  0.0        0.0      0.0   0.0  \n",
      "1    0.0   0.0          0.0  0.0      0.0   0.0  0.0        0.0      0.0   0.0  \n",
      "2    0.0   0.0          0.0  0.0      0.0   0.0  0.0        0.0      0.0   0.0  \n",
      "3    0.0   0.0          0.0  0.0      0.0   0.0  0.0        0.0      0.0   0.0  \n",
      "4    0.0   0.0          0.0  0.0      0.0   0.0  0.0        0.0      0.0   0.0  \n",
      "\n",
      "[5 rows x 2815 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiroto/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/Users/hiroto/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# データの分割\n",
    "train_valid = df[:len(train) + len(valid)]\n",
    "test = df[len(train) + len(valid):]\n",
    "\n",
    "# TfidfVectorizer\n",
    "vec_tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2))  # ngram_rangeでTF-IDFを計算する単語の長さを指定\n",
    "\n",
    "# ベクトル化\n",
    "X_train_valid = vec_tfidf.fit_transform(train_valid['TITLE'])  # testの情報は使わない\n",
    "X_test = vec_tfidf.transform(test['TITLE'])\n",
    "\n",
    "# ベクトルをデータフレームに変換\n",
    "X_train_valid = pd.DataFrame(X_train_valid.toarray(), columns=vec_tfidf.get_feature_names())\n",
    "X_test = pd.DataFrame(X_test.toarray(), columns=vec_tfidf.get_feature_names())\n",
    "\n",
    "# データの分割\n",
    "X_train = X_train_valid[:len(train)]\n",
    "X_valid = X_train_valid[len(train):]\n",
    "\n",
    "# データの保存\n",
    "# X_train.to_csv('./data/NewsAggregatorDataset/X_train.txt', sep='\\t', index=False)\n",
    "# X_valid.to_csv('./data/NewsAggregatorDataset/X_valid.txt', sep='\\t', index=False)\n",
    "# X_test.to_csv('./data/NewsAggregatorDataset/X_test.txt', sep='\\t', index=False)\n",
    "\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68477be",
   "metadata": {},
   "source": [
    "<h3>52: 51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a9bb285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, random_state=123)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, random_state=123)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000, random_state=123)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# モデルの学習\n",
    "model = LogisticRegression(random_state=123, max_iter=10000)\n",
    "model.fit(X_train, train['CATEGORY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22174e",
   "metadata": {},
   "source": [
    "<h3>53: 52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12d4dee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.84028172, 0.6790071 , 0.55636181, ..., 0.86051748, 0.61356376,\n",
      "       0.9082712 ]), array(['b', 't', 'm', ..., 'b', 'm', 'e'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def score_lg(model, X):\n",
    "  return [np.max(model.predict_proba(X), axis=1), model.predict(X)]\n",
    "\n",
    "train_pred = score_lg(model, X_train)\n",
    "test_pred = score_lg(model, X_test)\n",
    "\n",
    "print(train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce7b91",
   "metadata": {},
   "source": [
    "<h3>54: 52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d64ebbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率（学習データ）：0.927\n",
      "正解率（評価データ）：0.885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_accuracy = accuracy_score(train['CATEGORY'], train_pred[1])\n",
    "test_accuracy = accuracy_score(test['CATEGORY'], test_pred[1])\n",
    "print(f'正解率（学習データ）：{train_accuracy:.3f}')\n",
    "print(f'正解率（評価データ）：{test_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cd46b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
